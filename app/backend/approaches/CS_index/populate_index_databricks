
#This script is made for Databricks notebook
#%pip install azure-core
#%pip install azure-search-documents
#%pip install azure-storage-blob

import os
from typing import Dict
import base64
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes.models import (
    ComplexField,
    CorsOptions,
    SearchIndex,
    ScoringProfile,
    SearchFieldDataType,
    SimpleField,
    SearchableField
)
import json
from azure.storage.blob import BlobServiceClient
from multiprocessing import Pool

# Get the service endpoint and API key from the environment
endpoint = "<endpoint>" # "https://<search_service_name>.search.windows.net"
key = "<access key>"#search service key
index_name ="<index name>" #search index name 
connection_string = "<connection string blob>" #connection string to where the json files resides
container_name = "<container_name>" #name of container for json files



# Create a service client
search_client = SearchClient(endpoint, index_name, AzureKeyCredential(key))


def split_json(data, chunk_size):
    chunks = []
    start_idx = 0
    array_level = 0

    while start_idx < len(data):
        # Find the end index of the next JSON object
        end_idx = start_idx + chunk_size
        while end_idx < len(data):
            if data[end_idx] == '[':
                array_level += 1
            elif data[end_idx] == ']':
                array_level -= 1

            if array_level == 0 and data[end_idx] == ',':
                break

            end_idx += 1

        # Extract the JSON object and add it to the chunks
        chunk = data[start_idx:end_idx]
        chunks.append(chunk)

        start_idx = end_idx + 1

    return chunks


def extract_id_and_content(blob_content):
    data = json.loads(blob_content)
    id = data.get("id")
    content = json.dumps(data).replace('"', "'")
    return id, content


def extract_id(blob_content):
    data = json.loads(blob_content)
    id = data.get("id")
    return id


def encode_id(id_value):
    return id_value.replace(":", "_")


def process_blob(blob_name):
    # Get the blob client for the current blob
    blob_client = container_client.get_blob_client(blob_name)
    # Download the blob's content
    blob_content = blob_client.download_blob().readall()
    blob_name = blob_client.blob_name
    id, content = extract_id_and_content(blob_content)
    id = base64.urlsafe_b64encode(id.encode()).decode()
    #print('id: ' + id)
    output_fields = []
    result_split = split_json(json.dumps(json.loads(blob_content)), chunk_size)
    for i, chunk in enumerate(result_split):
        document = {
            "id": str(id),
            "content": chunk,
            "keyfield": encode_id(id) + "-" + str(i),
            "sourcefile": blob_name,
            "category": "",
            "sourcepage": blob_name + "-" + str(i)
        }
        output_fields.append(document)
    result = search_client.upload_documents(documents=output_fields)
    #print("Upload of new document succeeded: {}".format(result[0].succeeded))


if __name__ == '__main__':
    # Example usage
    chunk_size = 800

   
    # Create a BlobServiceClient object using the connection string
    blob_service_client = BlobServiceClient.from_connection_string(connection_string)


    # Get a reference to the container
    container_client = blob_service_client.get_container_client(container_name)

    # List all blobs (documents) in the container
    blob_list = container_client.list_blobs()
    counter = 0

    with Pool() as pool:
        pool.map(process_blob, [blob.name for blob in blob_list])

    exit()
